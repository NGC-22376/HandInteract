# 机器翻译基础

## 基本架构-Encoder-Decoder

1. Encoder

   - 输入嵌入层

     输入->密集向量。可用预训练的词向量（Word2Vec、GloVe...），或自己随机初始化后训练

   - 编码器层

     多层神经网络，词向量->特征。可用RNN、LSTM、GRU、Self-Attention Transformer架构。

   - 上下文向量

     特征->定长向量。

2. Decoder

   - 目标嵌入层

     输出->密集向量。这里的输出指的是已经生成的部分，对于训练阶段而言，是真实的前向序列，对于推理阶段而言，是模型自己预测的前向序列。

   - 解码器层

     多层神经网络，每一层内，对序列的每个时间步拆分，执行上一时间步输出+隐藏状态+本步表征向量->本步输出+本步隐藏状态。

     | **模型类型**           | **前一个时间步的输入来源**                                   | **隐藏状态 / 输出的依赖**                              |
     | ---------------------- | ------------------------------------------------------------ | ------------------------------------------------------ |
     | **RNN/LSTM/GRU**       | 序列中的前一个词元（训练和推理一致）                         | 依赖上一个时间步的隐藏状态（同一层内的时间依赖）       |
     | **Transformer 解码器** | **训练**：目标序列的前一个词元           **推理**：模型上一次生成的词元 | 依赖上一层的输出（层间依赖）+ 前序生成结果（时间依赖） |

   - 输出层

​		映射输出为每个词的概率输出。

## 词向量的构建方法

选择合适的词嵌入模型，能够直接生成基于语境的context vector

![image-20250621151628278](C:\Users\WenXin\AppData\Roaming\Typora\typora-user-images\image-20250621151628278.png)

## 子词分词算法

### BPE-贪心合并

给定语料库，初始拆分为最小单元，作为词表中的单元。而后不断检查词表中每两个单元在语料库中相邻的频率，不断合并词表中高相邻频率的单元，直至词表中单元数量达到预期值/所有单元的相邻频率均为1。

应用时，采用最长匹配的划分方式。

### WordPiece-基于概率的子词分词方法（BERT）

给定语料库，初始拆分为最小单元作为词表中的单元。当所有单元相互独立的时候，生成语料库的概率为单元出现在语料库中的概率之积。因此，模型任务为找到最合适的单元合并方式，以使语料库出现的概率最大。寻找过程中，不断计算合并两个子词的概率变化（合并后概率为p(AB)，独立单元的概率为p(A)p(B)），选择对概率增益超过某个阈值的单元合并。

应用时，采用动规以选择概率更高的子词划分方式。

### SentencePiece-BPE+Unigram（ChatGPT）

BPE方式生成候选子词，类似WordPiece的方式根据概率决策选择哪个子词进入词表。

对于非英文字符，用UTF-8编码作为最小分割单元。

# 大模型微调

## 大模型微调-PEFT

### LoRA：调节部分权重矩阵，适用于大型预训练语言模型

选择微调的权重矩阵（d\*d）后（一般是多头自注意/前馈神经网络），引入两个低秩矩阵A（d\*r）B（r\*d）（r<<d），新权重=原始权重+AB。

原始权重进行了调整，但改动不大。

### QLoRA：规范参数的精度，减少模型需要的存储空间和计算成本

每权重由4bit表示，选择量化范围后（如-1到1），将其分为16个区域，每个区域由4bit表示。量化时，将原始数映射到最接近的区域里。

训练时，以4bit形式加载模型并微调。

需要设计合适的映射和量化策略，不然精度损失会很严重。

### 适配器调整：原模型选定层间插入小型NN模块

维持预训练模型参数和结构不变，只训练适配器（小型模块）的参数

需要权衡加入的适配器的类型和位置

### 前缀调整：输入序列前添加前缀

预训练的语言模型前添加可训练、任务特定的前缀。

每个任务只需保存一套前缀，而非完整的模型参数。

### 提示调整：在输入序列中加入可学习向量

和prefix fine-tuning的区别在于，使用较少的向量模仿自然语言提示。而前缀调整提供的更多是输入的直接上下文信息，作为表意的一部分。

### P-Tuning：使用LSTM生成加入的可学习向量

复杂性比较高，要引入LSTM来处理更复杂的、顺序依赖的任务。但LSTM参数可共享，有更强的跨任务泛化能力。

### P-Tuning v2：在多层都插入生成的提示向量

适用于更深的网络、更复杂的任务

## 大模型微调-PILL

PEFT的一个实现，关注插入模块提升模型的能力

## 大模型微调-SSF

对模型特征进行缩放和位移，调整特征来提高模型的能力

# 思路

## 分割和词嵌入

更多可参考是的是SentencePiece算法，因为对于中文而言，它的起点是不具有人类语言学语义的编码，当语料库中所有的词都这样被表示之后，语义的显明性不再重要，只需根据统计规律来进行分割和合并即可。

但问题在于，即使是SentencePiece，编码也是基于单个字进行的，本身就是可以看成一个语义单元，如何拆分是一个大问题。

拆分到合并的过程，主要是为了在不影响语义的前提下减少词表中词的数量，合并的逻辑就是把经常一起出现的两个单元视为一体，因此，单个单元是否能够表意已经不再重要，很多时候截取的单元（苹...），也是无法单独表意的。重要的是，最后合并后的词表内的条目，均具有一定的表意能力。

这么想的话，按照一定的时间间隔划分电信号，作为词库最初的最小单元也是可行的。

但问题是，如何识别合并后词表的每一项，对应的是什么语义？如果不能确定的话，没法进行词嵌入。

有一种方法是绑定某一段电流信号到词义，但这样就相当于手工做子词分词，没用子词分割的成熟算法。

还有一种就是不用已有的词嵌入模型，自己训练。词嵌入的最终目标只是，词义越相近的输入词，输出的向量的余弦相似度就越高。但还是没能避免问题：如何确认输入的词义？

手语更像言语的图形表示，本身的拆分动作并不具有语义信息，还是依赖于言语的单字作为最小的表意单元。这么看，其实最可行的方案还是，手动进行子词分词，查目前已有的词表，将其中已有的言语词->密集向量的对应关系，更换成言语词对应的手语动作电信号->密集向量的对应关系。

这样，我们得到的数据相当于是手语词电流信号->语义，而已有的映射是语义->密集向量。使用”语义“作为中间桥梁，可以实现电流信号->密集向量的映射。

## Encoder

需要做的是，处理输入（batch,timesteps,10）为一个上下文向量(batch,T_enc,d_model)。相当于给模型的是一个batch的需处理数据，每组数据被分为T_enc个特征采样点，在一个特征采样点内有d_model个特征表征值。

线性映射层Linear将d_model映射为d_embed，适应于大模型输入的形状。

也就是说，这一层的目标只是，理解源信号究竟在表达什么，即”提取电流信号中的有义特征“。

**预训练模型的选择？微调策略？**

因为只是在提取特征，所以预训练模型没有语言相关的性质要求，只需选择具有序列信息处理能力的模型即可。此时就需要完全否定上述模拟子词分词的思想，因为那相当于把整个翻译任务交给了分词字典，肯定不行。

并不需要分割，手语语句的表意是一个整体，分割会破坏词与词之间的粘连（即上下文），Encoder要做的就是捕捉非定长序列中的特征，捕获长时依赖，理解整个序列从而获取最重要的特征。

## 大模型-Decoder

将每组数据对应的中文编码成(S,d_embed)的形状，拼接到每组数据后。

最终输入给大模型的形状就是(batch_size, T_enc + S, d_embed)（第二个维度：[信号向量h1, 信号向量h2, ..., 信号向量hT_enc, <特殊开始符>, 目标词1, 目标词2, ..., 目标词S-1]）。根据上文，S其实就是开始符+手语语句的文字对应的嵌入向量。

微调策略？

## 输出

损失函数选择什么？
